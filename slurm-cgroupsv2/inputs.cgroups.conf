# inputs.cgroup
#
[[inputs.cgroup]]
  # here we collect per slurm job cgroup stats
  paths = [
    "/sys/fs/cgroup/system.slice/slurmstepd.scope/job_*",
  ]
  files = [
    "memory.current",
    "memory.high",
    "memory.low",
    "memory.max",
    "memory.min",
    "memory.swap.current",
  ]

[[inputs.cgroup]]
  # here we collect systemwide cgroup stats
  # only the two metrics make sense..
  paths = [
    "/sys/fs/cgroup/system.slice/slurmstepd.scope/",
  ]
  files = [
    "memory.current",
    "memory.swap.current",
  ]


[[processors.starlark]]
  namepass = ['cgroup',]
  source = '''
load("logging.star", "log")
def apply(metric):
  cgroup_root_path=str(metric.tags['path'])
  # log.warn(str(cgroup_root_path))
  #log.warn(len(cgroup_root_path.split('/')))

  # parse job_id from cgroup_root_path if present, otherwise it's per-node stat:
  if ( len(cgroup_root_path.split('/')) >= 6 ) and  ( '_' in str(cgroup_root_path.split('/')[6]) ):
    # here we parse strings like /sys/fs/cgroup/system.slice/slurmstepd.scope/job_6991255
    #cgroup_uid=str(cgroup_root_path.split('/')[6]).split('_')[1]

    job_id=str(cgroup_root_path.split('/')[6]).split('_')[1]

    usage_in_bytes = float(metric.fields['memory.current'])
    limit_in_bytes = float(metric.fields['memory.max'])
    metric.fields['memory.current_percent'] = (usage_in_bytes / limit_in_bytes ) * 100

    #metric.tags['uid']=cgroup_uid
    metric.tags['job_id']=job_id
    metric.tags['nodeWide']='False'
  else:
    # here we parse strings like /sys/fs/cgroup/memory/slurm
    metric.tags['nodeWide']='True'

  # removed unused metric now:
  #metric.tags.pop("path")

  return metric
'''